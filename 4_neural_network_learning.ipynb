{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d8316364-3843-4962-b369-90d3682ee19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#使うライブラリ\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "# データセットの読み込み\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d43e0a0-cab5-4cfb-92d9-2f85bca67b33",
   "metadata": {},
   "source": [
    "# 4. ニューラルネットワークの学習"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6d3d4d-da9b-4506-97b9-cde0ea9db3db",
   "metadata": {},
   "source": [
    "Let's study neural network! ではなくニューラルネットワークにデータを与えて学習させるほう。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae873e46-536d-4068-954f-0a4051da3287",
   "metadata": {},
   "source": [
    "## 4.1関連：特徴量という考え方"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc1743c-d242-42a7-9279-1d59962dea00",
   "metadata": {},
   "source": [
    "[特徴量とは、一言で表すと、分析対象データの中の、予測の手掛かりとなる変数のことです。](https://www.sbbit.jp/article/cont1/76066)  \n",
    "また、データの何を変数として（特徴量として）使って機械学習を行おうかと、学習するためにデータを加工していく作業を特徴量エンジニアリング（feature engineering）ともいう。例えばデータセットの中に変数A,変数B,変数Cがあるとして、全てを機械学習アルゴリズムにかけるか、あるいはAとBだけというように一部だけ使うか、あるいは新たな変数を作るか（A＋BーC＝D的な）といった選択を行い、より精度の良いモデルを作ること。  \n",
    "  \n",
    "一般的な機械学習はデータ→特徴量を決める（特徴量エンジニアリング）→形の変わったデータ→機械学習の流れで行われる（粗い理解）。その一方でディープラーニングではその特徴量エンジニアリングがいらない。データをそのまま入力すると勝手にノードの重み（特徴量）を調整する。すごい。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c4cbe5-4ed8-4aa6-8da8-393d5ba78307",
   "metadata": {},
   "source": [
    "## 4.2 損失関数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a586ce3-218f-409d-ada3-8f99a639b2f0",
   "metadata": {},
   "source": [
    "最適な重みパラメーターの探索のために用いられる関数。主に2乗和誤差や交差エントロピー誤差が用いられる。損失関数を使ってモデルの予測が教師データとどれだけズレているかを算出し、そのズレを小さくしていくことがニューラルネットワークの学習になる。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ee409a-781f-4ce4-8d2b-37dc434d6918",
   "metadata": {},
   "source": [
    "### 2乗和誤差"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f2a920-01d5-47a6-be9a-74ef497f7a78",
   "metadata": {},
   "source": [
    "以下の計算でエラー(E)を求める。1/kをかけることもあるが、1/2とテキストでは記載されている。誤植ではなく[微分をしやすくするためらしい。](https://canplay-music.com/2019/06/02/loss-function/)  \n",
    "$$\n",
    "E = \\frac{1}{2}\\sum_{k}(y_k - t_k)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60b089e6-2234-4b6c-af79-f2a706f8a9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(y, t):\n",
    "    return 0.5 * np.sum((y-t)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7202954f-b509-4579-a00c-e34f83c39925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正解の予測の二乗和誤差の値\n",
      "0.09750000000000003\n",
      "\n",
      "間違いの予測の二乗和誤差の値\n",
      "0.5975\n"
     ]
    }
   ],
   "source": [
    "# 例\n",
    "\n",
    "##　yはニューラルネットからのソフトマックス関数を使っての出力のサンプル。1桁の数字の識別タスクと家庭。\n",
    "\n",
    "## 正解ラベルのワンホットベクトルでの表現。「２」が正解のベクトル。\n",
    "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "## ２を予測（せいかい）するベクトル。\n",
    "y1 = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
    "## 7を予測（まちがい）するベクトル\n",
    "y2 = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]\n",
    "\n",
    "# 正解を予測できている場合\n",
    "print(\"正解の予測の二乗和誤差の値\")\n",
    "print(mean_squared_error(np.array(y1), np.array(t)))\n",
    "print()\n",
    "print(\"間違いの予測の二乗和誤差の値\")\n",
    "print(mean_squared_error(np.array(y2), np.array(t)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013a24c3-1a01-4075-84d1-93c9abd0f517",
   "metadata": {},
   "source": [
    "上記のように、正解を出力できる方が間違いを出力する場合よりも二乗和誤差の数字が小さくなる。教師データ(t)により適合していることを示している。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d6fba5-3b30-494b-8843-64b8e090464b",
   "metadata": {},
   "source": [
    "### 交差エントロピー誤差"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301bb3e6-b8e5-488c-8d95-71d2ecf32818",
   "metadata": {},
   "source": [
    "以下の計算で誤差（E）を求める。logの底は自然対数。\n",
    "$$\n",
    "E = -\\sum_kt_k\\log y_k\n",
    "$$\n",
    "ここで、tは正解ラベルのワンホット表現（正解のインデックス以外は0）なので、実際は正解のインデックスに対応する出力の自然対数を計算する関数となっている。$\\log 1$は0なのでドンピシャに予測できると誤差がゼロになる。予測の確率が1より低くなると-で数値が大きくなる。たとえば$\\log 0.5$はおよそ-0.69で$\\log 0.1$はおよそ-2.3。マイナスを取るために総和のあとで-1がかけられている。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4d9d97c1-ac1c-4771-a307-7fcdb185454a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 実装\n",
    "def cross_entropy_error(y, t):\n",
    "    delta = 1e-7 # log 0を割けるために微小な数字を計算時に足す\n",
    "    return -np.sum(t * np.log(y + delta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1d58368a-a8c0-4509-841f-4dfa1dc7119e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正解の予測の交差エントロピー誤差の値\n",
      "0.510825457099338\n",
      "間違いの予測の交差エントロピー誤差の値\n",
      "2.302584092994546\n"
     ]
    }
   ],
   "source": [
    "# 例\n",
    "\n",
    "##　yはニューラルネットからのソフトマックス関数を使っての出力のサンプル。1桁の数字の識別タスクと家庭。\n",
    "\n",
    "## 正解ラベルのワンホットベクトルでの表現。「２」が正解のベクトル。\n",
    "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "## ２を予測（せいかい）するベクトル。\n",
    "y1 = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
    "## 7を予測（まちがい）するベクトル\n",
    "y2 = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]\n",
    "\n",
    "# 正解を予測できている場合\n",
    "print(\"正解の予測の交差エントロピー誤差の値\")\n",
    "print(cross_entropy_error(np.array(y1), np.array(t)))\n",
    "print(\"間違いの予測の交差エントロピー誤差の値\")\n",
    "print(cross_entropy_error(np.array(y2), np.array(t)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7d7859-2ef4-49c3-a09b-9b1d7e5e29cf",
   "metadata": {},
   "source": [
    "二乗和誤差のときと同様に、間違いの予測の値のほうが大きくなっているのがわかる。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974d6c53-ce84-427a-b4f6-667b8ec5d5af",
   "metadata": {},
   "source": [
    "### 4.2.3ミニバッチ学習"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c003f12f-5f6e-452b-95b7-830fdb5d7dc2",
   "metadata": {},
   "source": [
    "大量にある訓練データをある程度のまとまり（ミニバッチ）にしてそれをひとまとまりとして、ミニバッチごとに損失関数を計算する。\n",
    "これまで見てきた実装だとデータを一つ見て損失関数を計算していたが、ミニバッチだと大きな数のデータ（例えば100個とか）の損失関数を一度に計算するので、一つ一つ計算するよりも早い。  \n",
    "なお、一つずつ訓練データを使って損失関数を計算する手法をオンライン学習（Udemyではない）という。逆にすべての訓練データを使っていっぺんに損失関数を計算する手法をバッチ学習（ミニではない）という。ミニバッチ学習はバッチ学習とオンライン学習の中間のイメージ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6d507aeb-5dfa-42b7-ac46-6b4827e5a2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#　ミニバッチ学習（たとえば100個纏めて計算）出来るように交差エントロピー誤差の実装を変える\n",
    "\n",
    "def cross_entropy_error(y, t):\n",
    "    #データセットが一つだけの時に1次元から2次元に変形する必要がある。\n",
    "    if y.dim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(t * np.log(y + 1e-7)) / batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f650bbe-bc23-43aa-9ec0-b949a9eab074",
   "metadata": {},
   "source": [
    "## 微分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bf2f07-ba65-45e5-a4ac-66612b44adc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
